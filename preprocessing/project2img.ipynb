{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\"\"\"\n",
    "\n",
    "Created by Xin Zhang\n",
    "### References\n",
    "[1] Bashivan, et al. \"Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks.\" International conference on learning representations (2016).\n",
    "\n",
    "[2] https://github.com/numediart/EEGLearn-Pytorch/blob/master/Utils.py\n",
    "\n",
    "[3] Bagchi S, Bathula D R. EEG-ConvTransformer for single-trial EEG-based visual stimulus classification[J]. Pattern Recognition, 2022, 129: 108757.`\n",
    "\n",
    "Forked from https://github.com/numediart/EEGLearn-Pytorch/blob/master/Utils.py\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from preprocessing.aep import azim_proj, gen_images\n",
    "import einops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we generate fake EEG data to reduce time-cost. For re-train the model in citation, you can find the public dataset which download from https://purl.stanford.edu/bq914sc3730. other datasets are also supported as long as the following-like requirements are met, it's up to you.\n",
    "\n",
    "Single raw EEG data should be the shape: [Time, Channels] and a corresponding label y. Each channel/electrode has a 3D coordinate [X, Y, Z]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "num_samples = 64\n",
    "time = 2048\n",
    "channels = 32\n",
    "xyz_electrode = torch.randn(size=(channels, 3))\n",
    "eeg = torch.rand(size=(num_samples, time, channels)).cuda()\n",
    "labels = torch.randint(size=(num_samples,), high=19, low=0).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here We need employ FFT to extract power spectrum in each time-window\n",
    "In citation[1], It said: \"Fast Fourier Transform (FFT) is performed on the time series for each trial to estimate the power spectrum of the signal. Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (8-13Hz), and beta (13-30Hz) (Bashivan et al., 2014; Jensen & Tesche, 2002).\"\n",
    "\n",
    "Besides, there is a different between citation [1] and [3], you can find the details in [3], which described as:\n",
    "\"However, contrary to the three frequency power bands from the earlier work, the AEP and interpolation are applied to the preprocessed signal to form a single channel mesh of G1 Ã— G2 per time-frame.\"\n",
    "Based on the short describe, I guess the author may divide the EEG signal into many fragments, and then do FFT at each fragment to get the 'time-frame'. If you have other opinion and welcome to commit your code to re-correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_win = 128\n",
    "sample_rate = 1000\n",
    "eeg_ = einops.rearrange(eeg, 'n (f tw) c -> n f c tw ', n=num_samples, tw=time_win, c=channels)\n",
    "print(eeg_.shape)\n",
    "power = torch.abs(torch.fft.fft(eeg_, n=time_win, dim=-1, norm='forward'))\n",
    "freqs = torch.fft.fftfreq(n=time_win, d=1/sample_rate)\n",
    "theta_pass = torch.where((4 < freqs) & (freqs <= 7), True, False)\n",
    "alpha_pass = torch.where((8 < freqs) & (freqs <= 13), True, False)\n",
    "beta_pass = torch.where((13 < freqs) & (freqs <= 30), True, False)\n",
    "\n",
    "theta = power[:, :, :, theta_pass]\n",
    "alpha = power[:, :, :, alpha_pass]\n",
    "beta = power[:, :, :, beta_pass]\n",
    "print(theta.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"Sum of squared absolute values within each of the three frequency bands was computed and used as separate measurement for each electrode.\"\n",
    "Now we have three tensor with shape [num_samples, frame, channels]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta = torch.norm(theta, p=2, dim=-1, keepdim=False)\n",
    "alpha = torch.norm(alpha, p=2, dim=-1, keepdim=False)\n",
    "beta = torch.norm(beta, p=2, dim=-1, keepdim=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Making channels features that can be found in their official code[2]\n",
    "\"Features are arranged in band and electrodes order (theta_1, theta_2..., theta_64, alpha_1, alpha_2, ..., beta_64).\"\n",
    "Then the features shape will be [num_samples, frame, channels*3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = torch.cat([theta, alpha, beta], dim=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember We have the electrodes coordinate and label that defined before.\n",
    "Now, we cast 3D coordinate(xyz) into 2D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "locs_2d = []\n",
    "for e in xyz_electrode:\n",
    "    locs_2d.append(azim_proj(e))\n",
    "print(np.shape(locs_2d))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, it's time to generate images. We need to generate image for each sample and also each time-frame. So we can perform:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feats = einops.rearrange(features, 'n f c3 -> (n f) c3')\n",
    "images = []\n",
    "for i in range(feats.shape[1]):\n",
    "    img = gen_images(locs=np.array(locs_2d),\n",
    "                     features=feats,\n",
    "                     n_gridpoints=32,\n",
    "                     normalize=True)\n",
    "    images.append(img)\n",
    "images_time_win = torch.Tensor(np.array(images))\n",
    "time_frame_3d = einops.rearrange(images, '(n f) color m m -> n f color m m')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[num_samples, frame, color, m, m]\n",
    "consider to average the color dimension since the input of EEG-ConvTransformer is [batch, time-frame, 1, m, m]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_frame_3d = torch.mean(time_frame_3d, dim=2, keepdim=True)\n",
    "sio.savemat(\"sample_data/time_frames.mat\",{\"img\":time_frame_3d})\n",
    "sio.savemat(\"sample_data/labels.mat\",{\"lab\":labels})"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
